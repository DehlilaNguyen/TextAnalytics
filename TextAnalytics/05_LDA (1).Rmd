---
title: "Topic Models"
author: "Dehlila Nguyen"
date: '2023-06-01'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE)
```

#### Latent Dirichlet Allocation (LDA) is the popular topic model available to extract the topics from text documents. 


- Topic modeling is a probability-based approach to finding clusters within documents. It is unsupervised because you do not have document-assigned classes like "positive" or "negative" for IMDB movie reviews.       
- When working on finding topics within text documents, there are two distinct probabilities. If there are "K" topics, "D" documents, and "W" words that make all of these "D" documents, then there is a word probability that defines a topic (a set of words makes a topic or a topic can be explained using a set of words). Word1 is highly likely to describe topic 1, but it may be less likely to explain topic 2 than it was previously. The second probability is the likelihood of a given document (a sentence or set of sentences) being associated with a given topic.

![](Images/LDA1.jpg)

- The words describing the topics are hidden, and the topics within the documents are hidden (latent), which is why the algorithm is called LDA.     
- Gibbs Sampling is the most commonly used sampling method for identifying topics and word probabilities.               
- A Markov chain Monte Carlo (MCMC) algorithm is used in the Gibbs sampler. It approximates distributions, then generates correlated Markov Chain samples for statistical inference.      
- The topic model inference yields two (approximate) posterior probability distributions: theta over K topics within each document and beta over V terms within each topic, where V represents the collection's vocabulary length.    

![](Images/LDA2.jpg)
```{r 1. Import the required libraries}
library(tidytext)
library(tm)
library(topicmodels)
library(ldatuning)
library(textmineR)
library(tidyverse)
library(readxl)
library(textcat)
library(stringr)
```

- Read the dataset consisting of Blutooth speaker reviews from Amazon    

```{r 2. load the dataset}
text <- read_xlsx(paste0(getwd(), "/BlutoothSpeaker_B09JB8KPNW.xlsx"))
```

```{r 3. basic text processing function for reusable code}
text_pre_process <- function(corpus) {
  corpus <- tm_map(corpus, tolower)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"))
  return(corpus)
}

```


```{r 4. call_text_processing code}

text <- text %>%
  mutate(language = textcat(review_text)) %>%
  filter(language == "english") %>%
  mutate(review_text <- str_replace_all(review_text, "[[:punct:]]", ""))

corpus <- Corpus(VectorSource(text$review_text))
corpus <- text_pre_process(corpus)
text_tm_processed <- corpus$content
text_tm_processed <- str_squish(text_tm_processed)
text_tm_processed <- as.data.frame(text_tm_processed)
names(text_tm_processed) <- c("text")

text_tm_processed <- data.frame(text = text_tm_processed[1:500, "text"] )
```


```{r 5. Run the LDA model using the LDA function}
corpus <- Corpus(VectorSource(text_tm_processed$text))
dtm <- DocumentTermMatrix(corpus 
                  
                   )
# Include frequency terms that appear in at least 50 documents 
dtm <- dtm[,findFreqTerms(dtm,50)]

# Include the dtm rows that contain non-zero rows
nonzero_row_numbers <- slam::row_sums(dtm) > 0

# Create updated dtm
dtm <- dtm[nonzero_row_numbers, ]
```


```{r 6. Run fine tuning step }
finetune_result <- FindTopicsNumber(
  dtm,
  topics = seq(2, 20, by = 2),
  control = list(seed = 1234)
)
```


```{r 7. Plot the optimal LDA topics}
FindTopicsNumber_plot(finetune_result)
```


```{r 8. Run LDA model with the optimal number of topics}
K <- 14
topicModel <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, 
                                                       verbose = 50))
```


- Let's look at the posterior estimates. 
```{r 9. Interpreting LDA result}
LDA_Result <- posterior(topicModel)
attributes(LDA_Result)

# topics are probability distributions over the entire vocabulary
beta <- LDA_Result$terms   # get beta from results
dim(beta)                # K distributions over nTerms(dtm) terms

# Row sums by topic should equal to 1
rowSums(beta)

nDocs(dtm)


# for every document we have a probability distribution of its contained topics
theta <- LDA_Result$topics 
dim(theta)

# Sum of probabilities of each document 
rowSums(theta)[1:5] 


# Terms distribution for the first 5 topics. 
term_distribution <- terms(topicModel, 10)
term_distribution[, 1:5]

# Concatenate the terms to give a good name for each topic. 
topic_names <- apply(terms(topicModel, 3), 2, 
                     FUN = paste, collapse = " ")
```


```{r 10. wordcloud of topics}
topic_sel <- 5 # select topic 5 to visualize 

#topic_sel <- grep('ear', topic_names)[1] # selecting a topic by a term 
# select to 50 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top50terms <- sort(LDA_Result$terms[topic_sel,], decreasing=TRUE)[1:50]
words <- names(top50terms)

# extract the probabilites of each of the 50 terms
probabilities <- sort(LDA_Result$terms[topic_sel,], decreasing=TRUE)[1:50]
# visualize the terms as wordcloud
wordcloud(words, probabilities, random.order = FALSE)
```


```{r 12. A tidy LDA model}
tidy_lda_output <- tidy(topicModel, matrix = "beta")

```

